{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terms & Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "3 steps:\n",
    "1. Cleaning\n",
    "2. Transformation\n",
    "    * Features normalization\n",
    "        * Min-max scaling $$X'=a+{\\frac {\\left(X-X_{\\min }\\right)\\left(b-a\\right)}{X_{\\max }-X_{\\min }}}$$ where $a$ and $b$ are boundary values\n",
    "    * One-hot encoding\n",
    "3. [Dimentionality] reduction\n",
    "    * Principal Component Analysis (PCA)\n",
    "        * Normalize the data\n",
    "        * Compute covariance matrix $$\\sum=\\frac{1}{n-1}\\left((X-\\bar x)^T(X-\\bar x)\\right)$$\n",
    "        * Eigen decomposition\n",
    "        * Create projection matrix\n",
    "        * Squash features with projection matrix\n",
    "    * T-SNE\n",
    "    * LDA (Linear Discriminant Analysis)\n",
    "4. Data randomization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "* linear $$Wx+b$$\n",
    "* rectified linear unit (ReLU) $$f(x) = \\max(x, 0)$$\n",
    "* sigmoid $$sigmoid = \\frac{1}{1 + e^{-x}} = 1 - \\frac{1}{1 + e^x}$$ derivative $$\\frac{d}{dx}(sigmoid)=\\frac{e^x}{(1+e^x)^2}=\\frac{1}{1 + e^x}\\left(1 - \\frac{1}{1 + e^x}\\right)=sigmoid(1-sigmoid)$$\n",
    "* softmax $$\\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^K{e^{z_k}}} \\quad \\text{for } j = 1, \\dots, K$$ where $z$ is a $K$-dimentional vector and $\\sigma(z)$ is a \"squashed\" vector of the same dimention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost functions\n",
    "* sum of squared errors (SSE)\n",
    "* cross entropy - for classification with one-hot encoded labels $$\\hat y = \\left\\lgroup \\matrix{0.1\\cr 0.5\\cr 0.4}\\right\\rgroup \\quad y = \\left\\lgroup \\matrix{0\\cr 1\\cr 0}\\right\\rgroup$$  $$D(\\hat y, y)=-\\sum_j{y_j\\ln{\\hat y_j}}$$\n",
    "Loss function: $$L=\\frac{1}{N} \\sum_i{D(S(Wx_i + b), L_i)}$$ where $D$ is cross entropy loss and $S$ is softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning optimization\n",
    "* weights initialization with random values from truncated ($\\sigma$) normal distribution\n",
    "* back propagation $$w \\leftarrow w - \\alpha \\Delta_w L$$ $$b \\leftarrow b - \\alpha \\Delta_b L$$\n",
    "* gradient descent\n",
    "* stochastic gradient descent (SGD)\n",
    "    * momentum - running average for SGD: $$M \\leftarrow 0.9M + \\Delta L$$\n",
    "    * learning rate decay - lowering learning rate during SGD\n",
    "* ADAGRAD - SGD which implicitly does momentum and learning rate decay\n",
    "* Mini-batching (optimize memory consumption, computationally inefficietn)\n",
    "* early termination\n",
    "* regularizatoin \n",
    "    * L2 Regularization $$L' = L + \\beta\\frac{1}{2}\\|w\\|^2_2$$ where $\\beta$ is a small constant (hyperparameter), and $$\\frac{1}{2}\\|w\\|^2_2 = \\frac{1}{2}(w^2_1 + w^2_2 + \\dots + w^2_n)$$\n",
    "    * dropout\n",
    "\n",
    "For convolutional:\n",
    "* pooling\n",
    "    * max pooling\n",
    "    * average pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques\n",
    "* Wight sharing (statistical invariants)\n",
    "* Word2Vec\n",
    "    * Skip-gram\n",
    "    * CBOW (Continuous Bag-Of-Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "* learning rate $\\alpha$\n",
    "* number of layers and neurons in each layer\n",
    "* batch size\n",
    "* number of epochs\n",
    "* $\\beta$ (L2 Regularization constant)\n",
    "* stride (convolutional networks)\n",
    "* depth of CNN filter $k$\n",
    "* pooling region size (convolutional networks)\n",
    "* pooling region stride (convolutional networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network layer calculation\n",
    "* convolutional layer output shape: $$(Wâˆ’F+2P)/S+1$$ where $W$ is volume of input layer, $F$ volume of filter ($\\text{height}\\times\\text{width}\\times\\text{depth}$), $S$ is stride and $P$ is padding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
